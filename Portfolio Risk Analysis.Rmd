---
title: "Risk Analysis for FA 800"
author: "Jahnavi Ravi"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r}
# Load necessary libraries
library(quantmod)
library(PerformanceAnalytics)
library(PortfolioAnalytics)
library(xts)
library(knitr)
```


```{r}
symbols <- c("AMGN", "NEE", "NVDA", "MCD", "V")

get_tic_function <- function(x) {
  getSymbols(x, from = "2014-01-01", to = "2024-01-01")
  return(get(x))
}
P_list <- lapply(symbols, get_tic_function)
length(P_list)
```

```{r}
# Retrieve the adjusted prices
get_adj_price <- function(x) Ad(x)
P_adj_list <- lapply(P_list, get_adj_price)

# Merge adjusted prices for all symbols
P <- do.call(merge, P_adj_list)

# Check the resulting merged data frame
names(P) <- symbols
head(P)
```
```{r}
# Convert P to xts object
P_xts <- as.xts(P)

# Apply apply.weekly function to aggregate to weekly frequency and keep only the last observation of each week
P_weekly <- apply.weekly(P_xts, last)

# Check the head of the resulting weekly data
head(P_weekly)
```

```{r}
# First, we'll convert the adjusted prices to log returns
log_returns <- na.omit(diff(log(P_weekly)))
log_returns <- log_returns["2014/2024",]

# Check the resulting merged data frame
names(log_returns) <- symbols
head(log_returns)
```

```{r}
# Compute mean return, volatility, and Sharpe ratio
Mu_A <- 52 * apply(log_returns, 2, mean)
Sig_A <- sqrt(52) * apply(log_returns, 2, sd)
SR_A <- Mu_A / Sig_A
result <- data.frame(cbind(Mu_A, Sig_A, SR_A))
colnames(result) <- c("Mean", "Volatility", "SR")
round(result, 2)
```
# The mean return for either ETF should be consistent with the mean annual return for each.

```{r}
# Convert to yearly data
P_A <- apply.yearly(P_weekly, last)

# Compute the returns
R_A <- na.omit(log(P_A / lag(P_A)))

# Compute the mean return
Mu_A2 <- colMeans(R_A, na.rm = TRUE)

# Create a data frame with mean returns
result_annual <- data.frame(Mean = round(Mu_A2, 2))

# Print the result
print(result_annual)

# Combine mean annual and weekly data into a dataframe and round to 2 decimal places
mean_data <- data.frame(Annual_Mean = round(Mu_A2, 2), Weekly_Scaled_Mean = round(Mu_A, 2))

# Print the rounded mean_data dataframe
print(mean_data)
```
The results are roughly the same, with difference, which can be attributed to the fact that the average number of weeks during this sample period was not exactly 52 but rather

```{r}
# Calculate average number of weeks
avg_weeks <- mean(apply.yearly(log_returns["2014/2024"], nrow))
avg_weeks
```


```{r}
# Calculate standard deviations
Sig_A2 <- apply(R_A, 2, sd)
Sig_A3 <- sqrt(avg_weeks) * apply(log_returns, 2, sd)

# Create a dataframe with both annual and scaled standard deviations, rounded to 2 decimal places
volatility_data <- data.frame(Annual_Volatility = round(Sig_A2, 2), Weekly_Scaled_Volatility = round(Sig_A3, 2))
print(volatility_data)
```

### For volatility, on the other hand, this is not necessarily the case. One explanation for this is the assumption of independence. For mean returns, it does not matter. For this reason, we get the same result regardless of how we computed returns. On the other hand, for variance, it does matter since the IID assumption ignores potential correlation among weekly returns over time. Specifically, looking at weekly returns over this period, we note that returns do exhibit serial correlation. 


```{r}
# Calculate correlation between AMGN and its lagged series
cor_amgn <- cor(log_returns$AMGN, lag(log_returns$AMGN), use = "pairwise")

# Calculate correlation between NEE and its lagged series
cor_nee <- cor(log_returns$NEE, lag(log_returns$NEE), use = "pairwise")

# Calculate correlation between NVDA and its lagged series
cor_nvda <- cor(log_returns$NVDA, lag(log_returns$NVDA), use = "pairwise")

# Calculate correlation between MCD and its lagged series
cor_mcd <- cor(log_returns$MCD, lag(log_returns$MCD), use = "pairwise")

# Calculate correlation between V and its lagged series
cor_v <- cor(log_returns$V, lag(log_returns$V), use = "pairwise")

# Create a dataframe with correlation results
correlation_data <- data.frame(Symbol = c("AMGN", "NEE", "NVDA", "MCD", "V"),
                                Correlation = c(cor_amgn, cor_nee, cor_nvda, cor_mcd, cor_v))

# Print the correlation dataframe
print(correlation_data)
```

```{r}
# Define stock names
stock_names <- c("AMGN", "NEE", "NVDA", "MCD", "V")

# Define Gamma_mat as a diagonal matrix
Gamma_mat <- diag(Sig_A)

# Define Lambda_mat as the correlation matrix
Lambda_mat <- cor(log_returns)

# Calculate Sig_mat_A2
Sig_mat_A2 <- Gamma_mat %*% Lambda_mat %*% Gamma_mat

# Assign row and column names
colnames(Sig_mat_A2) <- rownames(Sig_mat_A2) <- stock_names

# Print Covariance Matrix
print("Covariance Matrix:")
print(Sig_mat_A2)
```

```{r}
# Define stock names
stock_names <- c("AMGN", "NEE", "NVDA", "MCD", "V")

# Function to calculate portfolio mean and volatility
w_function <- function(w) {
  w_vec <- matrix(w, nrow = length(w), ncol = 1)
  mu_p <- t(w_vec) %*% Mu_A
  sig_p <- sqrt(t(w_vec) %*% Sig_mat_A2 %*% w_vec)
  result <- c(mu_p, sig_p)
  return(result)
}

# Generate a sequence of portfolio weights (grid)
w_seq <- expand.grid(seq(0, 1, by = 0.1), seq(0, 1, by = 0.1), seq(0, 1, by = 0.1), seq(0, 1, by = 0.1), seq(0, 1, by = 0.1))
w_seq <- w_seq[rowSums(w_seq) == 1, ]

# Remove duplicate rows
w_seq <- unique(w_seq)

# Apply the w_function to each weight vector
ds <- t(apply(w_seq, 1, w_function))
ds <- data.frame(ds)
names(ds) <- c("mu_p", "sig_p")

# Plot the efficient frontier
plot(mu_p ~ sig_p, data = ds, type = "p", xlab = expression(sigma[p]), ylab = expression(mu[p]))
```


```{r}
# Create a data frame with portfolio risk and return values
portfolio_data <- data.frame(Risk = round(ds$sig_p, 3), Return = round(ds$mu_p, 3))

# Subset to include only the first 5 rows
portfolio_data_subset <- portfolio_data[1:10, ]

# Display the subsetted data frame as a table
kable(portfolio_data_subset)

summary(ds)
```


```{r}
# Define stock names
stock_names <- c("AMGN", "NEE", "NVDA", "MCD", "V")

# Calculate w_0 and w_1
vec_ones <- rep(1, nrow(Sig_mat_A2))
w_0 <- solve(Sig_mat_A2) %*% vec_ones
w_0 <- w_0 / sum(w_0)
B_mat <- solve(Sig_mat_A2) %*% (diag(vec_ones) - vec_ones %*% t(w_0))
w_1 <- B_mat %*% Mu_A

# Function to calculate portfolio mean and volatility
w_A_function <- function(A) {
  w_vec <- w_0 + (1/A) * w_1
  mu_p <- t(w_vec) %*% Mu_A
  sig_p <- sqrt(t(w_vec) %*% Sig_mat_A2 %*% w_vec)
  result <- c(mu_p, sig_p)
  return(result)
}

# Generate a sequence of risk aversion values
A_seq <- seq(1, 100, length = 100)

# Apply the w_A_function to each risk aversion value
ds_A <- t(sapply(A_seq, w_A_function))
ds_A <- data.frame(ds_A)
names(ds_A) <- c("mu_p", "sig_p")

# Plot the efficient frontier
plot(mu_p ~ sig_p, data = ds, type = "p", xlab = expression(sigma[p]), ylab = expression(mu[p]))
lines(mu_p ~ sig_p, data = ds_A, col = "red", lty = 2, lwd = 2)
```

```{r}
# Print the global minimum variance portfolio weights (w_0)
cat("Global Minimum Variance Portfolio Weights:\n")
print(w_0)
cat("\n")
```

